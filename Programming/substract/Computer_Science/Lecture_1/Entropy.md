Энтропи́я (от др.-греч. ἐν «в» + τροπή «обращение; превращение») — широко используемый в естественных и точных науках термин (впервые введён в рамках термодинамики как функция состояния термодинамической системы), обозначающий меру необратимого рассеивания энергии или бесполезности энергии (потому что не всю энергию системы можно использовать для превращения в какую-нибудь полезную работу). Для понятия энтропии в данном разделе физики используют название термодинамическая энтропия; термодинамическая энтропия обычно применяется для описания равновесных (обратимых) процессов.

В статистической физике энтропия характеризует вероятность осуществления какого-либо макроскопического состояния. Кроме физики, термин широко употребляется в математике: теории информации и математической статистике. В этих областях знания энтропия определяется статистически и называется статистической или информационной энтропией. Данное определение энтропии известно также как энтропия Шеннона (в математике) и энтропия Больцмана—Гиббса (в физике).

Хотя понятия термодинамической и информационной энтропии вводятся в рамках различных формализмов, они имеют общий физический смысл — логарифм числа доступных микросостояний системы. Взаимосвязь этих понятий впервые установил Людвиг Больцман. В неравновесных (необратимых) процессах энтропия также служит мерой близости состояния системы к равновесному: чем больше энтропия, тем ближе система к равновесию (в состоянии термодинамического равновесия энтропия системы максимальна).

В широком смысле, в каком слово часто употребляется в быту, энтропия означает меру сложности, хаотичности или неопределённости системы: чем меньше элементы системы подчинены какому-либо порядку, тем выше энтропия.

Величина, противоположная энтропии, именуется негэнтропией или, реже, экстропией.

